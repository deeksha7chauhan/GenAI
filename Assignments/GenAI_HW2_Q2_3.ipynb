{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. Explore the core components of LangChain (LLMs, Prompt Templates, Chains, etc). Experiment with each and describe how they interact"
      ],
      "metadata": {
        "id": "icjo4n9ai_Mg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59096fd3",
        "outputId": "2591abda-55a5-48b1-ca75-25f9a45be3d4"
      },
      "source": [
        "%pip install -U langchain-openai"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.33-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.76 (from langchain-openai)\n",
            "  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.106.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (0.4.24)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.104.2->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-openai) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.5.0)\n",
            "Downloading langchain_openai-0.3.33-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.76-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain-openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.75\n",
            "    Uninstalling langchain-core-0.3.75:\n",
            "      Successfully uninstalled langchain-core-0.3.75\n",
            "Successfully installed langchain-core-0.3.76 langchain-openai-0.3.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. LLM**"
      ],
      "metadata": {
        "id": "mQ_d_Bo-sS0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "llm_cold = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0): This creates an instance of the ChatOpenAI model.\n",
        "model=\"gpt-4o-mini\": Specifies the particular OpenAI model to use"
      ],
      "metadata": {
        "id": "wi40RG5w6NRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "import os\n",
        "# Get the API key from Colab's secrets manager\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('DC')\n",
        "\n",
        "llm_cold = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) ## low randomness Deterministic\n",
        "llm_warm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.8) # more creative\n",
        "print(llm_cold.invoke(\"In one line, what is LangChain?\")) # Changed llm to llm_cold to use one of the created models\n",
        "print(llm_warm.invoke(\"In one line, what is LangChain?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8zCUUpzmAxJ",
        "outputId": "90f93c45-33af-42cb-8254-65b56d3613e7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='LangChain is a framework designed to facilitate the development of applications using language models by providing tools for chaining together various components and functionalities.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 16, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFv8khilPp0NurqMVBB7Z0GGcynmv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--c465ee92-5373-4e05-ac84-a587acf76e6c-0' usage_metadata={'input_tokens': 16, 'output_tokens': 26, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "content='LangChain is a framework designed for building applications that leverage large language models (LLMs) by integrating them with various data sources and tools.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 16, 'total_tokens': 44, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFv8lDix0Dwgs6XpMBsXVrXBtpfRQ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--7d6f0e74-211c-4ea0-a766-d063294972fb-0' usage_metadata={'input_tokens': 16, 'output_tokens': 28, 'total_tokens': 44, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt Templates,**"
      ],
      "metadata": {
        "id": "bL-8mjFqsba6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are concise.\"),\n",
        "    (\"human\", \"Summarize in 12 words: {text}\")\n",
        "])\n",
        "\n",
        "print(prompt.format(text=\"LCEL lets you pipe prompt→model→parser cleanly.\"))"
      ],
      "metadata": {
        "id": "q97vf-DRmP5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d5806ee-a0ba-414e-b6b0-bb3b984afed0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: You are concise.\n",
            "Human: Summarize in 12 words: LCEL lets you pipe prompt→model→parser cleanly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**### Chains**"
      ],
      "metadata": {
        "id": "LEzKhsVFIwx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "cold_chain = prompt | llm_cold | StrOutputParser()\n",
        "warm_chain = prompt | llm_warm | StrOutputParser()\n",
        "\n",
        "print(cold_chain.invoke({\"text\": \"LangChain enables modular LLM apps.\"}))\n",
        "time.sleep(21)\n",
        "print(warm_chain.invoke({\"text\": \"LangChain enables modular LLM apps.\"}))\n",
        "time.sleep(21)\n",
        "\n",
        "# Batch: many inputs at once (must be a list)\n",
        "outs = warm_chain.batch([\n",
        "    {\"text\": \"LCEL composes steps.\"},\n",
        "    {\"text\": \"Prompt → Model → Parser.\"},\n",
        "])\n",
        "print(outs)\n",
        "\n",
        "# Stream: iterate chunks as they arrive\n",
        "for chunk in warm_chain.stream({\"text\": \"Stream this reply in parts.\"}):\n",
        "    print(chunk, end=\"\")\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS3K-bGNdAJz",
        "outputId": "80a1b8c7-b657-480e-e4e7-f4fbd1d6a52c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain facilitates the development of modular applications using large language models.\n",
            "LangChain facilitates the creation of modular applications using language models efficiently.\n",
            "['LCEL outlines a structured approach for composing and organizing steps efficiently.', 'Input prompt guides model processing, output parsed for structured interpretation.']\n",
            "Sure! Please provide the reply you want summarized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tools (function calling)**\n",
        "\n",
        "Give the model callable utilities; it decides when to use them.**"
      ],
      "metadata": {
        "id": "7ZeRua3WdXjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "@tool\n",
        "def time_in_tz(tz: str) -> str:\n",
        "    \"\"\"Return current time in IANA timezone (e.g., 'Europe/Paris').\"\"\"\n",
        "    return datetime.now(pytz.timezone(tz)).isoformat()\n",
        "\n",
        "tool_llm = llm_cold.bind_tools([time_in_tz])\n",
        "print(tool_llm.invoke(\"What's the exact time in Europe/Paris?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWFdsMY6dP08",
        "outputId": "58293e70-f715-4614-8aaf-f86c76f18c85"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='' additional_kwargs={'tool_calls': [{'id': 'call_Wq4o5G13W09oUHahdI5isaVa', 'function': {'arguments': '{\"tz\":\"Europe/Paris\"}', 'name': 'time_in_tz'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 64, 'total_tokens': 82, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CFvDgYAP4ndWcEnN8Xz9t5RiTlzd2', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--7d75f98c-4941-468d-b684-9a9bb117140a-0' tool_calls=[{'name': 'time_in_tz', 'args': {'tz': 'Europe/Paris'}, 'id': 'call_Wq4o5G13W09oUHahdI5isaVa', 'type': 'tool_call'}] usage_metadata={'input_tokens': 64, 'output_tokens': 18, 'total_tokens': 82, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Output Parser structured results***"
      ],
      "metadata": {
        "id": "Ru5d5K0vdnKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pydantic\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough # Import RunnablePassthrough\n",
        "\n",
        "class CityPlan(BaseModel):\n",
        "    city: str = Field(..., description=\"destination\")\n",
        "    attractions: list[str] = Field(..., description=\"top 3\")\n",
        "\n",
        "parser = JsonOutputParser(pydantic_object=CityPlan)\n",
        "\n",
        "json_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Return valid JSON: {format_instructions}\"), # Changed from format to format_instructions\n",
        "    (\"human\", \"Plan a short trip to {place}.\")\n",
        "])\n",
        "\n",
        "json_chain = (\n",
        "    RunnablePassthrough.assign(format_instructions=lambda x: parser.get_format_instructions()) # Use RunnablePassthrough\n",
        "    | json_prompt | llm_cold | parser # Assuming llm_cold is defined\n",
        ")\n",
        "print(json_chain.invoke({\"place\": \"Kyoto\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIdEgZdtdll3",
        "outputId": "cc6ae308-d212-4872-ab4a-833dab62d689"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'city': 'Kyoto', 'attractions': ['Kinkaku-ji (Golden Pavilion)', 'Fushimi Inari Taisha', 'Arashiyama Bamboo Grove']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**###Q3. Explore following optimization techniques that can be performed while training and document your findings with a basic example code snippets:**\n",
        "\n",
        "\n",
        "1.  Tensor Creation (CPU vs GPU)\n",
        "2.  Weight Initialization\n",
        "3.  Activation Checkpointing\n",
        "4. Gradient Accumulation\n",
        "5. Mixed Precision Training**"
      ],
      "metadata": {
        "id": "SIvRuTVFeJwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time, math\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.checkpoint import checkpoint_sequential\n",
        "from contextlib import nullcontext\n",
        "\n",
        "torch.manual_seed(42)\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using device:', DEVICE)\n",
        "if DEVICE == 'cuda':\n",
        "    print('The CUDA capability:', torch.cuda.get_device_name(0))\n",
        "\n"
      ],
      "metadata": {
        "id": "5fNH7SjeeJm1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ef1e80-bed5-47f6-bc66-5c3fb33cb3d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "The CUDA capability: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bench_tensor_ops(n=4096, repeats=5):\n",
        "    times = {}\n",
        "    for dev in ['cpu'] + (['cuda'] if torch.cuda.is_available() else []):\n",
        "        # warmup\n",
        "        a = torch.randn((n,n), device=dev)\n",
        "        b = torch.randn((n,n), device=dev)\n",
        "        c = a @ b\n",
        "        if dev=='cuda': torch.cuda.synchronize()\n",
        "        # timed\n",
        "        t_acc = 0.0\n",
        "        for _ in range(repeats):\n",
        "            t0 = time.time()\n",
        "            a = torch.randn((n,n), device=dev)\n",
        "            b = torch.randn((n,n), device=dev)\n",
        "            c = a @ b\n",
        "            if dev=='cuda': torch.cuda.synchronize()\n",
        "            t_acc += time.time() - t0\n",
        "        times[dev] = t_acc / repeats\n",
        "    return times\n",
        "\n",
        "times = bench_tensor_ops(n=1024, repeats=3)\n",
        "print('Avg. Time: for 1024x1024 matmul):', times)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNniR01dHIJl",
        "outputId": "b4585bed-0d51-4ef2-ce9a-4347da0a87a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. Time: for 1024x1024 matmul): {'cpu': 0.0417013963063558, 'cuda': 0.0009594758351643881}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight Initialization"
      ],
      "metadata": {
        "id": "bhiIdWYsFjUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F, torch.nn.init as init\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(0); np.random.seed(0)\n",
        "\n",
        "class TinyMLP(nn.Module):\n",
        "    def __init__(self, d_in=784, d_hidden=512, d_out=10, depth=4):\n",
        "        super().__init__()\n",
        "        dims = [d_in] + [d_hidden]*depth + [d_out]\n",
        "        self.layers = nn.ModuleList([nn.Linear(dims[i], dims[i+1]) for i in range(len(dims)-1)])\n",
        "    def forward(self, x):\n",
        "        for lin in self.layers[:-1]: x = F.relu(lin(x))\n",
        "        return self.layers[-1](x)\n",
        "\n",
        "def init_weights(model, mode):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            if mode == \"xavier\": init.xavier_uniform_(m.weight)\n",
        "            elif mode == \"he\":   init.kaiming_uniform_(m.weight, nonlinearity=\"relu\")\n",
        "            if m.bias is not None: init.zeros_(m.bias)\n",
        "\n",
        "def activation_stats(model, n_samples=256, d_in=784):\n",
        "    x = torch.randn(n_samples, d_in); out = []\n",
        "    with torch.no_grad():\n",
        "        for lin in model.layers[:-1]:\n",
        "            x = F.relu(lin(x))\n",
        "            a = x.detach().cpu().numpy()\n",
        "            out.append((a.mean(), a.std()))\n",
        "    return out\n",
        "\n",
        "def print_block(title, stats):\n",
        "    print(f\"=== init: {title} ===\")\n",
        "    for i,(m,s) in enumerate(stats, 1):\n",
        "        print(f\"[ L{i:02d} ] μ={m: .4f} | σ={s: .4f}\")\n",
        "    print(\"-\"*32)\n",
        "\n",
        "for mode in [\"default\",\"xavier\",\"he\"]:\n",
        "    mlp = TinyMLP()\n",
        "    if mode != \"default\": init_weights(mlp, mode)\n",
        "    stats = activation_stats(mlp)\n",
        "    print_block(mode, stats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMNl7hzhkUIp",
        "outputId": "9b24777a-fb07-42fd-9850-7b490d7d6ab8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== init: default ===\n",
            "[ L01 ] μ= 0.2292 | σ= 0.3372\n",
            "[ L02 ] μ= 0.0963 | σ= 0.1396\n",
            "[ L03 ] μ= 0.0418 | σ= 0.0605\n",
            "[ L04 ] μ= 0.0207 | σ= 0.0299\n",
            "--------------------------------\n",
            "=== init: xavier ===\n",
            "[ L01 ] μ= 0.4379 | σ= 0.6409\n",
            "[ L02 ] μ= 0.2889 | σ= 0.4427\n",
            "[ L03 ] μ= 0.2167 | σ= 0.3157\n",
            "[ L04 ] μ= 0.1546 | σ= 0.2207\n",
            "--------------------------------\n",
            "=== init: he ===\n",
            "[ L01 ] μ= 0.5619 | σ= 0.8228\n",
            "[ L02 ] μ= 0.5702 | σ= 0.8382\n",
            "[ L03 ] μ= 0.5666 | σ= 0.8259\n",
            "[ L04 ] μ= 0.5153 | σ= 0.7817\n",
            "--------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation Checkpointing"
      ],
      "metadata": {
        "id": "g-MtNxN-FoPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.checkpoint import checkpoint_sequential\n",
        "\n",
        "torch.manual_seed(0)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def sync():\n",
        "    if DEVICE == \"cuda\": torch.cuda.synchronize()\n",
        "\n",
        "class DeepMLP(nn.Module):\n",
        "    def __init__(self, depth=16, width=1024, d_in=1024, d_out=10):\n",
        "        super().__init__()\n",
        "        layers = [nn.Linear(d_in, width), nn.ReLU()]\n",
        "        for _ in range(depth - 2):\n",
        "            layers += [nn.Linear(width, width), nn.ReLU()]\n",
        "        layers += [nn.Linear(width, d_out)]\n",
        "        self.seq = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.seq(x)\n",
        "\n",
        "def forward_ckpt(seq, x, segments):\n",
        "    x = x.requires_grad_(True)\n",
        "    return checkpoint_sequential(seq, segments, x, use_reentrant=False)\n",
        "\n",
        "def train_step(model, x, y, segments=None, lr=1e-3):\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    out = forward_ckpt(model.seq, x, segments) if segments else model(x)\n",
        "    loss = F.cross_entropy(out, y)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    return float(loss.detach())\n",
        "\n",
        "def bench(model, x, y, segments=None):\n",
        "    _ = train_step(model, x, y, segments)  # warmup\n",
        "    sync()\n",
        "    if DEVICE == \"cuda\": torch.cuda.reset_peak_memory_stats()\n",
        "    t0 = time.time()\n",
        "    loss = train_step(model, x, y, segments)\n",
        "    sync()\n",
        "    t1 = time.time()\n",
        "    mb = torch.cuda.max_memory_allocated() / 1e6 if DEVICE == \"cuda\" else float(\"nan\")\n",
        "    return loss, (t1 - t0), mb\n",
        "\n",
        "N, D, C = 2048, 1024, 10\n",
        "x = torch.randn(N, D, device=DEVICE)\n",
        "y = torch.randint(0, C, (N,), device=DEVICE)\n",
        "\n",
        "model_a = DeepMLP().to(DEVICE)\n",
        "loss_a, time_a, mem_a = bench(model_a, x, y, segments=None)\n",
        "\n",
        "model_b = DeepMLP().to(DEVICE)\n",
        "loss_b, time_b, mem_b = bench(model_b, x, y, segments=8)\n",
        "\n",
        "print(f\"Device: {DEVICE}\")\n",
        "if DEVICE == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"{'Mode':<22}{'Loss':>10}{'Time(s)':>12}{'Peak MB':>12}\")\n",
        "print(\"-\" * 56)\n",
        "print(f\"{'No checkpointing':<22}{loss_a:>10.4f}{time_a:>12.3f}{mem_a:>12.0f}\")\n",
        "print(f\"{'With checkpointing':<22}{loss_b:>10.4f}{time_b:>12.3f}{mem_b:>12.0f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqSRnkhdkdf3",
        "outputId": "88d4682f-ccf5-4193-8c74-9d9c60d72022"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "GPU: Tesla T4\n",
            "Mode                        Loss     Time(s)     Peak MB\n",
            "--------------------------------------------------------\n",
            "No checkpointing          2.3014       0.081         341\n",
            "With checkpointing        2.3014       0.083         475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Accumulation"
      ],
      "metadata": {
        "id": "2EMMczyiFq3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.checkpoint import checkpoint_sequential\n",
        "\n",
        "torch.manual_seed(0)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def sync():\n",
        "    if DEVICE == \"cuda\": torch.cuda.synchronize()\n",
        "\n",
        "class DeepMLP(nn.Module):\n",
        "    def __init__(self, depth=16, width=1024, d_in=1024, d_out=10):\n",
        "        super().__init__()\n",
        "        layers = [nn.Linear(d_in, width), nn.ReLU()]\n",
        "        for _ in range(depth - 2):\n",
        "            layers += [nn.Linear(width, width), nn.ReLU()]\n",
        "        layers += [nn.Linear(width, d_out)]\n",
        "        self.seq = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.seq(x)\n",
        "\n",
        "def forward_ckpt(seq, x, segments):\n",
        "    x = x.requires_grad_(True)\n",
        "    return checkpoint_sequential(seq, segments, x, use_reentrant=False)\n",
        "\n",
        "def train_step(model, x, y, segments=None, lr=1e-3):\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    out = forward_ckpt(model.seq, x, segments) if segments else model(x)\n",
        "    loss = F.cross_entropy(out, y)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    return float(loss.detach())\n",
        "\n",
        "def bench(model, x, y, segments=None):\n",
        "    _ = train_step(model, x, y, segments)  # warmup\n",
        "    sync()\n",
        "    if DEVICE == \"cuda\": torch.cuda.reset_peak_memory_stats()\n",
        "    t0 = time.time()\n",
        "    loss = train_step(model, x, y, segments)\n",
        "    sync()\n",
        "    t1 = time.time()\n",
        "    mb = torch.cuda.max_memory_allocated() / 1e6 if DEVICE == \"cuda\" else float(\"nan\")\n",
        "    return loss, (t1 - t0), mb\n",
        "\n",
        "N, D, C = 2048, 1024, 10\n",
        "x = torch.randn(N, D, device=DEVICE)\n",
        "y = torch.randint(0, C, (N,), device=DEVICE)\n",
        "\n",
        "model_a = DeepMLP().to(DEVICE)\n",
        "loss_a, time_a, mem_a = bench(model_a, x, y, segments=None)\n",
        "\n",
        "model_b = DeepMLP().to(DEVICE)\n",
        "loss_b, time_b, mem_b = bench(model_b, x, y, segments=8)\n",
        "\n",
        "print(f\"Device: {DEVICE}\")\n",
        "if DEVICE == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"{'Mode':<22}{'Loss':>10}{'Time(s)':>12}{'Peak MB':>12}\")\n",
        "print(\"-\" * 56)\n",
        "print(f\"{'No checkpointing':<22}{loss_a:>10.4f}{time_a:>12.3f}{mem_a:>12.0f}\")\n",
        "print(f\"{'With checkpointing':<22}{loss_b:>10.4f}{time_b:>12.3f}{mem_b:>12.0f}\")\n"
      ],
      "metadata": {
        "id": "bNEJN9_ykd24"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mixed Precision Training"
      ],
      "metadata": {
        "id": "DmjqFKaTFvO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# assumes TinyMLP and DEVICE are already defined above\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model = TinyMLP().to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "BATCH = 128\n",
        "MICRO = 32\n",
        "acc_steps = BATCH // MICRO  # 4 micro-batches per update\n",
        "\n",
        "def grad_acc_step():\n",
        "    model.train()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    avg_loss = 0.0\n",
        "    for _ in range(acc_steps):\n",
        "        xb = torch.randn(MICRO, 784, device=DEVICE)\n",
        "        yb = torch.randint(0, 10, (MICRO,), device=DEVICE)\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb) / acc_steps\n",
        "        loss.backward()\n",
        "        avg_loss += float(loss.item())  # sum of scaled losses == average unscaled loss\n",
        "    optimizer.step()\n",
        "    return avg_loss\n",
        "\n",
        "loss_acc = grad_acc_step()\n",
        "print('Gradient accumulation step loss (avg):', loss_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e_hejnxkgF1",
        "outputId": "37e450d1-decd-4ed6-a377-7e4ad27c5faf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient accumulation step loss (avg): 2.3042796850204468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assumes TinyMLP and DEVICE are already defined above\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "use_amp = (DEVICE == \"cuda\")\n",
        "print(f\"Device={DEVICE} | AMP={'on' if use_amp else 'off'}\")\n",
        "\n",
        "model = TinyMLP().to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
        "\n",
        "def amp_step():\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    x = torch.randn(256, 784, device=DEVICE)\n",
        "    y = torch.randint(0, 10, (256,), device=DEVICE)\n",
        "    with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    return float(loss)\n",
        "\n",
        "t0 = time.time()\n",
        "loss = amp_step()\n",
        "if DEVICE == \"cuda\":\n",
        "    torch.cuda.synchronize()\n",
        "t1 = time.time()\n",
        "\n",
        "print(f\"[AMP] loss={loss:.4f} | elapsed={t1 - t0:.4f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEBZEsn7Ij98",
        "outputId": "8260a50c-1858-47e6-db3c-f1e0105bc9ee"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device=cuda | AMP=on\n",
            "[AMP] loss=2.2958 | elapsed=0.1565s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1054451677.py:25: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
            "  return float(loss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "/diabetes (1).csv"
      ],
      "metadata": {
        "id": "VDuUs7F8Nno5"
      }
    }
  ]
}