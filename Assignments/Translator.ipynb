{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q2. 2 Seq2Seq Transformer model En-Fr Fr-En\n",
        "\n",
        "Load your two files\n",
        "Tokenizers: spaCy (en_core_web_sm, fr_core_news_sm)\n",
        "Build vocabs\n",
        "Create a Dataset + collate_fn\n",
        "Train two models: EN→FR and FR→EN (same architecture, just swap source/target)\n",
        "Do round-trip: novel EN → FR (model1), then FR → EN (model2).\n",
        "Read pairs (line-by-line EN–FR). Make word lists (tokenize) and a dictionary (vocab) for EN and FR. Turn words into numbers (tensor sequences). Build the same Transformer twice (same architecture). Teach EN➜FR with (EN as input, FR as target). Teach FR➜EN with (FR as input, EN as target). Try a new English sentence, translate to French, then back to English."
      ],
      "metadata": {
        "id": "NzAR3_Dout7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io, math, random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import spacy"
      ],
      "metadata": {
        "id": "n4miqeVjsyMn"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "mm47yZur6SWk"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA"
      ],
      "metadata": {
        "id": "hvWTWy-VtDVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EN_FILE = \"/content/english.txt\"\n",
        "FR_FILE = \"/content/french.txt\""
      ],
      "metadata": {
        "id": "wLuSbwW4s_Bs"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_VOCAB = 20000     # cap vocab size (used for both EN & FR)\n",
        "MAX_LEN   = 64       # truncate long sentences\n",
        "BATCH_SIZE= 128\n",
        "EPOCHS    = 5        # used for both EN→FR and FR→EN\n",
        "LR        = 2e-4\n",
        "\n",
        "D_MODEL   = 256      # model width\n",
        "NHEAD     = 4        # attention heads\n",
        "LAYERS    = 3        # encoder & decoder layers\n",
        "VALID_RATIO  = 0.02\n",
        "MAX_VOCAB_EN = MAX_VOCAB_FR = MAX_VOCAB\n",
        "ENC_LAYERS   = DEC_LAYERS   = LAYERS\n",
        "DIM_FF       = 4 * D_MODEL\n",
        "DROPOUT      = 0.1"
      ],
      "metadata": {
        "id": "N-LvieSCtCey"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# 2) Read paired data from two files, keep alignment**"
      ],
      "metadata": {
        "id": "1B317RgMtdAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_parallel_two_files(en_path, fr_path):\n",
        "    with io.open(en_path, \"r\", encoding=\"utf-8\") as fe:\n",
        "        en_lines = [l.strip() for l in fe]\n",
        "    with io.open(fr_path, \"r\", encoding=\"utf-8\") as ff:\n",
        "        fr_lines = [l.strip() for l in ff]\n",
        "    n = min(len(en_lines), len(fr_lines))\n",
        "    if len(en_lines) != len(fr_lines):\n",
        "        print(f\"Note: length mismatch EN={len(en_lines)} FR={len(fr_lines)} → using first {n} aligned pairs\")\n",
        "    return list(zip(en_lines[:n], fr_lines[:n]))\n",
        "\n",
        "pairs = read_parallel_two_files(EN_FILE, FR_FILE)\n",
        "random.shuffle(pairs)\n",
        "n_valid = max(1, int(len(pairs) * VALID_RATIO))\n",
        "valid_pairs = pairs[:n_valid]\n",
        "train_pairs = pairs[n_valid:]\n",
        "print(f\"Train: {len(train_pairs)}  Valid: {len(valid_pairs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXCIshN2tOSw",
        "outputId": "e954112e-80b6-45d2-d1b0-f7bacd02a2cc"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 151786  Valid: 3097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asBhKneoscUe",
        "outputId": "da1f0376-4408-45bf-f059-1a53f5534b8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m122.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip -q install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# 3) spaCy tokenizers**"
      ],
      "metadata": {
        "id": "YB3ZgpvXyA6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) spaCy tokenizers\n",
        "nlp_en = spacy.load(\"en_core_web_sm\",\n",
        "                    disable=[\"tagger\",\"parser\",\"ner\",\"attribute_ruler\",\"lemmatizer\"])\n",
        "nlp_fr = spacy.load(\"fr_core_news_sm\",\n",
        "                    disable=[\"tagger\",\"parser\",\"ner\",\"attribute_ruler\",\"lemmatizer\"])"
      ],
      "metadata": {
        "id": "iglnIGhosgcU"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# 4) Vocab build (separate EN & FR) with special tokens aligned**"
      ],
      "metadata": {
        "id": "2PMrZQA1x8t6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PAD, BOS, EOS, UNK = \"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"\n",
        "\n",
        "def tokenize_corpus(nlp, texts, batch_size=1000, n_process=2):\n",
        "    \"\"\"Return list[list[str]] of tokens for each text.\"\"\"\n",
        "    out = []\n",
        "    # If n_process causes issues on your Colab (rare), set n_process=1\n",
        "    for doc in nlp.pipe(texts, batch_size=batch_size, n_process=n_process):\n",
        "        out.append([t.text.lower() for t in doc if not (t.is_space or t.is_punct)])\n",
        "    return out\n",
        "\n",
        "# Build vocab from token lists (no spaCy here)\n",
        "def build_vocab_from_tokens(tokens_list, max_size):\n",
        "    from collections import Counter\n",
        "    cnt = Counter(tok for toks in tokens_list for tok in toks)\n",
        "    # sort by freq desc, then alpha for tie-break\n",
        "    items = sorted(cnt.items(), key=lambda x: (-x[1], x[0]))\n",
        "    tokens = [PAD, BOS, EOS, UNK] + [w for w,_ in items]\n",
        "    if len(tokens) > max_size:\n",
        "        tokens = tokens[:max_size]\n",
        "    stoi = {w:i for i,w in enumerate(tokens)}\n",
        "    itos = tokens\n",
        "    return stoi, itos\n"
      ],
      "metadata": {
        "id": "4tLPj8TTx5f8"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5) Encode to ID lists**"
      ],
      "metadata": {
        "id": "w8Hx8easyPEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Encode to ID lists (truncate to MAX_LEN-2; BOS/EOS added later in batching)\n",
        "def encode_from_tokens(tokens_list, stoi, max_len):\n",
        "    unk = stoi[UNK]\n",
        "    out = []\n",
        "    for toks in tokens_list:\n",
        "        ids = [stoi.get(tok, unk) for tok in toks][:max_len-2]  # room for BOS/EOS in batching\n",
        "        out.append(ids)\n",
        "    return out\n",
        "\n",
        "en_train_texts = [en for en,_ in train_pairs]\n",
        "fr_train_texts = [fr for _,fr in train_pairs]\n",
        "en_valid_texts = [en for en,_ in valid_pairs]\n",
        "fr_valid_texts = [fr for _,fr in valid_pairs]\n",
        "\n",
        "train_en_tokens = tokenize_corpus(nlp_en, en_train_texts, batch_size=1000, n_process=1)\n",
        "train_fr_tokens = tokenize_corpus(nlp_fr, fr_train_texts, batch_size=1000, n_process=1)\n",
        "valid_en_tokens = tokenize_corpus(nlp_en, en_valid_texts, batch_size=1000, n_process=1)\n",
        "valid_fr_tokens = tokenize_corpus(nlp_fr, fr_valid_texts, batch_size=1000, n_process=1)\n",
        "\n",
        "# Build vocabs from TRAIN tokens only (special tokens aligned)\n",
        "en_stoi, en_itos = build_vocab_from_tokens(train_en_tokens, MAX_VOCAB_EN)\n",
        "fr_stoi, fr_itos = build_vocab_from_tokens(train_fr_tokens, MAX_VOCAB_FR)\n",
        "PAD_ID = en_stoi[PAD]; BOS_ID = en_stoi[BOS]; EOS_ID = en_stoi[EOS]; UNK_ID = en_stoi[UNK]\n",
        "\n",
        "# Encode to ID lists\n",
        "train_en_ids = encode_from_tokens(train_en_tokens, en_stoi, MAX_LEN)\n",
        "train_fr_ids = encode_from_tokens(train_fr_tokens, fr_stoi, MAX_LEN)\n",
        "valid_en_ids = encode_from_tokens(valid_en_tokens, en_stoi, MAX_LEN)\n",
        "valid_fr_ids = encode_from_tokens(valid_fr_tokens, fr_stoi, MAX_LEN)"
      ],
      "metadata": {
        "id": "w0kWAp5dx50W"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# 6) Batch builder: pad + masks**"
      ],
      "metadata": {
        "id": "4SZNPgv8yiRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batch(id_lists_src, id_lists_tgt, pad_id=PAD_ID):\n",
        "    B = len(id_lists_src)\n",
        "    src_lens = [len(x) for x in id_lists_src]\n",
        "    tgt_lens = [len(y) for y in id_lists_tgt]\n",
        "    S = max(1, max(src_lens))\n",
        "    T = max(1, max(tgt_lens)) + 1  # +1 for BOS/EOS shift\n",
        "\n",
        "    src    = torch.full((B, S), pad_id, dtype=torch.long, device=device)\n",
        "    tgt_in = torch.full((B, T), pad_id, dtype=torch.long, device=device)\n",
        "    tgt_out= torch.full((B, T), pad_id, dtype=torch.long, device=device)\n",
        "\n",
        "    for i, (s, t) in enumerate(zip(id_lists_src, id_lists_tgt)):\n",
        "        s = s[:S]; t = t[:T-1]\n",
        "        src[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n",
        "        ti = [BOS_ID] + t\n",
        "        to = t + [EOS_ID]\n",
        "        tgt_in[i, :len(ti)] = torch.tensor(ti, dtype=torch.long)\n",
        "        tgt_out[i, :len(to)] = torch.tensor(to, dtype=torch.long)\n",
        "\n",
        "    src_pad = (src == pad_id)    # (B,S) bool\n",
        "    tgt_pad = (tgt_in == pad_id) # (B,T) bool\n",
        "    Tlen = tgt_in.size(1)\n",
        "    tgt_sub = torch.triu(torch.ones((Tlen, Tlen), dtype=torch.bool, device=device), diagonal=1)  # (T,T) bool\n",
        "    return src, tgt_in, tgt_out, src_pad, tgt_pad, tgt_sub"
      ],
      "metadata": {
        "id": "42j1gbWvygLb"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Model: embeddings + sinusoidal PE + nn.Transformer\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):  # x: (L,B,E)\n",
        "        x = x + self.pe[:x.size(0)].unsqueeze(1)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, src_vocab, tgt_vocab, d_model, nhead, enc_layers, dec_layers, dim_ff, dropout, pad_id):\n",
        "        super().__init__()\n",
        "        self.src_emb = nn.Embedding(src_vocab, d_model, padding_idx=pad_id)\n",
        "        self.tgt_emb = nn.Embedding(tgt_vocab, d_model, padding_idx=pad_id)\n",
        "        self.pos = PositionalEncoding(d_model, dropout)\n",
        "        self.tr = nn.Transformer(d_model=d_model, nhead=nhead,\n",
        "                                 num_encoder_layers=enc_layers, num_decoder_layers=dec_layers,\n",
        "                                 dim_feedforward=dim_ff, dropout=dropout)\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, src, tgt_in, src_pad, tgt_pad, tgt_sub):\n",
        "        s = self.src_emb(src) * math.sqrt(self.d_model)\n",
        "        t = self.tgt_emb(tgt_in) * math.sqrt(self.d_model)\n",
        "        s = self.pos(s.transpose(0,1))\n",
        "        t = self.pos(t.transpose(0,1))\n",
        "        out = self.tr(s, t,\n",
        "                      src_key_padding_mask=src_pad,\n",
        "                      tgt_key_padding_mask=tgt_pad,\n",
        "                      tgt_mask=tgt_sub,\n",
        "                      memory_key_padding_mask=src_pad)     # (T,B,E)\n",
        "        return self.fc(out)  # (T,B,V)"
      ],
      "metadata": {
        "id": "OSmnBBz2yz0d"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Mini-batch iterator, train/valid loops\n",
        "def iterate_minibatches(N, batch_size=BATCH_SIZE, shuffle=True):\n",
        "    idx = torch.arange(N)\n",
        "    if shuffle:\n",
        "        idx = idx[torch.randperm(N)]\n",
        "    for s in range(0, N, batch_size):\n",
        "        e = min(s+batch_size, N)\n",
        "        yield idx[s:e].tolist()\n",
        "\n",
        "def train_epoch(model, opt, src_ids, tgt_ids):\n",
        "    model.train()\n",
        "    crit = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
        "    total, toks = 0.0, 0\n",
        "    N = len(src_ids)\n",
        "    for b in iterate_minibatches(N, BATCH_SIZE, shuffle=True):\n",
        "        src, tgt_in, tgt_out, src_pad, tgt_pad, tgt_sub = make_batch(\n",
        "            [src_ids[i] for i in b], [tgt_ids[i] for i in b]\n",
        "        )\n",
        "        logits = model(src, tgt_in, src_pad, tgt_pad, tgt_sub)     # (T,B,V)\n",
        "        T,B,V = logits.shape\n",
        "        loss = crit(logits.view(T*B, V), tgt_out.transpose(0,1).reshape(T*B))\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        tokens = (tgt_out != PAD_ID).sum().item()\n",
        "        total += loss.item() * tokens\n",
        "        toks  += tokens\n",
        "    return total / max(1, toks)\n",
        "\n",
        "@torch.no_grad()\n",
        "def valid_epoch(model, src_ids, tgt_ids):\n",
        "    model.eval()\n",
        "    crit = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
        "    total, toks = 0.0, 0\n",
        "    N = len(src_ids)\n",
        "    for b in iterate_minibatches(N, BATCH_SIZE, shuffle=False):\n",
        "        src, tgt_in, tgt_out, src_pad, tgt_pad, tgt_sub = make_batch(\n",
        "            [src_ids[i] for i in b], [tgt_ids[i] for i in b]\n",
        "        )\n",
        "        logits = model(src, tgt_in, src_pad, tgt_pad, tgt_sub)\n",
        "        T,B,V = logits.shape\n",
        "        loss = crit(logits.view(T*B, V), tgt_out.transpose(0,1).reshape(T*B))\n",
        "        tokens = (tgt_out != PAD_ID).sum().item()\n",
        "        total += loss.item() * tokens\n",
        "        toks  += tokens\n",
        "    return total / max(1, toks)"
      ],
      "metadata": {
        "id": "LQJN-YUzy5CB"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# 9) Greedy decoding + translate helpers**"
      ],
      "metadata": {
        "id": "DPOj4c9x6fmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9) Greedy decoding + translate helpers\n",
        "@torch.no_grad()\n",
        "def beam_search_decode(model, src_ids, beam_size=4, alpha=0.6, max_len=MAX_LEN):\n",
        "    def lp(l): return ((5 + l)**alpha) / ((5 + 1)**alpha)  # length penalty\n",
        "    src = torch.tensor(src_ids, dtype=torch.long, device=device).unsqueeze(0)  # (1,S)\n",
        "    src_pad = (src == PAD_ID)\n",
        "\n",
        "    beams = [([BOS_ID], 0.0, False)]  # (ids, sum_logprob, finished)\n",
        "    completed = []\n",
        "\n",
        "    for _ in range(max_len - 1):\n",
        "        new_beams = []\n",
        "        for ids, logp, done in beams:\n",
        "            if done:\n",
        "                new_beams.append((ids, logp, True))\n",
        "                continue\n",
        "            ys = torch.tensor(ids, dtype=torch.long, device=device).unsqueeze(0)  # (1,t)\n",
        "            T = ys.size(1)\n",
        "            tgt_pad = (ys == PAD_ID)\n",
        "            tgt_sub = torch.triu(torch.ones((T, T), dtype=torch.bool, device=device), diagonal=1)\n",
        "            logits = model(src, ys, src_pad, tgt_pad, tgt_sub)          # (T,1,V)\n",
        "            log_probs = torch.log_softmax(logits[-1, 0], dim=-1)        # (V,)\n",
        "            topk = torch.topk(log_probs, beam_size)\n",
        "            for next_id, lp_token in zip(topk.indices.tolist(), topk.values.tolist()):\n",
        "                ids2 = ids + [next_id]\n",
        "                done2 = (next_id == EOS_ID)\n",
        "                logp2 = logp + lp_token\n",
        "                new_beams.append((ids2, logp2, done2))\n",
        "\n",
        "        new_beams.sort(key=lambda x: (x[1] / lp(len(x[0]))), reverse=True)\n",
        "        beams = new_beams[:beam_size]\n",
        "\n",
        "        still_alive = []\n",
        "        for b in beams:\n",
        "            if b[2]: completed.append(b)\n",
        "            else:    still_alive.append(b)\n",
        "        beams = still_alive or beams\n",
        "        if len(completed) >= beam_size:\n",
        "            break\n",
        "\n",
        "    pool = completed if completed else beams\n",
        "    best = max(pool, key=lambda x: (x[1] / lp(len(x[0]))))\n",
        "    seq = best[0][1:]  # drop BOS\n",
        "    if EOS_ID in seq:\n",
        "        seq = seq[:seq.index(EOS_ID)]\n",
        "    return seq\n",
        "\n",
        "def decode(ids, itos):\n",
        "    toks = []\n",
        "    for i in ids:\n",
        "        if 0 <= i < len(itos):\n",
        "            tok = itos[i]\n",
        "            if tok not in (PAD, BOS, EOS):\n",
        "                toks.append(tok)\n",
        "    return \" \".join(toks)\n",
        "\n",
        "# Tokenize a single sentence (for demo translation)\n",
        "def tok_en_line(s): return [t.text.lower() for t in nlp_en(s) if not (t.is_space or t.is_punct)]\n",
        "def tok_fr_line(s): return [t.text.lower() for t in nlp_fr(s) if not (t.is_space or t.is_punct)]\n",
        "\n",
        "def translate(text, model, src_stoi, single_tok, tgt_itos):\n",
        "    src_tokens = single_tok(text)\n",
        "    src_ids = [src_stoi.get(tok, src_stoi[UNK]) for tok in src_tokens][:MAX_LEN-2]\n",
        "    hyp_ids = beam_search_decode(model, src_ids, beam_size=4, alpha=0.6, max_len=MAX_LEN)\n",
        "    return decode(hyp_ids, tgt_itos)\n"
      ],
      "metadata": {
        "id": "vlAWVTazy7Hn"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# 10) Build + train EN→FR**"
      ],
      "metadata": {
        "id": "Vwc_ccwZ6AJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10) Build + train EN→FR\n",
        "model_en2fr = Seq2SeqTransformer(\n",
        "    src_vocab=len(en_itos), tgt_vocab=len(fr_itos),\n",
        "    d_model=D_MODEL, nhead=NHEAD, enc_layers=ENC_LAYERS, dec_layers=DEC_LAYERS,\n",
        "    dim_ff=DIM_FF, dropout=DROPOUT, pad_id=PAD_ID\n",
        ").to(device)\n",
        "# weight tying\n",
        "model_en2fr.fc.weight = model_en2fr.tgt_emb.weight\n",
        "opt_en2fr = torch.optim.Adam(model_en2fr.parameters(), lr=LR)\n",
        "\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr = train_epoch(model_en2fr, opt_en2fr, train_en_ids, train_fr_ids)\n",
        "    va = valid_epoch(model_en2fr, valid_en_ids, valid_fr_ids) if len(valid_en_ids)>0 else float(\"nan\")\n",
        "    print(f\"[EN→FR] epoch {ep}  train_loss/token {tr:.4f}  valid {va:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xjquQxNzBd9",
        "outputId": "feeb509f-3e72-431c-c0a4-c9c1bc32c677"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EN→FR] epoch 1  train_loss/token 11.6192  valid 5.3265\n",
            "[EN→FR] epoch 2  train_loss/token 5.2600  valid 4.0034\n",
            "[EN→FR] epoch 3  train_loss/token 4.1844  valid 3.4154\n",
            "[EN→FR] epoch 4  train_loss/token 3.5604  valid 2.9681\n",
            "[EN→FR] epoch 5  train_loss/token 3.1319  valid 2.6699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# 11) Build + train FR→EN**"
      ],
      "metadata": {
        "id": "q4x0WPGrzM7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 11) Build + train FR→EN\n",
        "model_fr2en = Seq2SeqTransformer(\n",
        "    src_vocab=len(fr_itos), tgt_vocab=len(en_itos),\n",
        "    d_model=D_MODEL, nhead=NHEAD, enc_layers=ENC_LAYERS, dec_layers=DEC_LAYERS,\n",
        "    dim_ff=DIM_FF, dropout=DROPOUT, pad_id=PAD_ID\n",
        ").to(device)\n",
        "model_fr2en.fc.weight = model_fr2en.tgt_emb.weight\n",
        "opt_fr2en = torch.optim.Adam(model_fr2en.parameters(), lr=LR)\n",
        "\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr = train_epoch(model_fr2en, opt_fr2en, train_fr_ids, train_en_ids)\n",
        "    va = valid_epoch(model_fr2en, valid_en_ids, valid_en_ids) if len(valid_en_ids)>0 else float(\"nan\")\n",
        "    print(f\"[FR→EN] epoch {ep}  train_loss/token {tr:.4f}  valid {va:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4WHvPzazC3c",
        "outputId": "73216423-2f0c-4c5b-d254-ca2f873ae0e0"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FR→EN] epoch 1  train_loss/token 10.5338  valid 6.8422\n",
            "[FR→EN] epoch 2  train_loss/token 4.7973  valid 6.4979\n",
            "[FR→EN] epoch 3  train_loss/token 3.7762  valid 6.3417\n",
            "[FR→EN] epoch 4  train_loss/token 3.1658  valid 6.3552\n",
            "[FR→EN] epoch 5  train_loss/token 2.7618  valid 6.5700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# 12) Examples: EN→FR then feed into FR→EN**"
      ],
      "metadata": {
        "id": "B2W-h_aEzJ0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    \"hello how are you\",\n",
        "    \"this is a great day today\",\n",
        "    \"we test round trip translation\",\n",
        "    \"I will help you whenever needed\"\n",
        "]\n",
        "for en in examples:\n",
        "    fr_hat = translate(en, model_en2fr, en_stoi, tok_en_line, fr_itos)\n",
        "    en_back = translate(fr_hat, model_fr2en, fr_stoi, tok_fr_line, en_itos)\n",
        "    print(f\"\\nEN: {en}\")\n",
        "    print(f\"FR(hat): {fr_hat}\")\n",
        "    print(f\"EN(back): {en_back}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIv72OFrzFzu",
        "outputId": "c328d8c7-cc2c-4bb5-b9f8-a4ad3ce5309b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EN: hello how are you\n",
            "FR(hat): comment êtes -vous\n",
            "EN(back): how did you get\n",
            "\n",
            "EN: this is a great day today\n",
            "FR(hat): c' est une journée aujourd'hui\n",
            "EN(back): today is a day today\n",
            "\n",
            "EN: we test round trip translation\n",
            "FR(hat): nous nous avons passer le voyage\n",
            "EN(back): we have a trip trip\n",
            "\n",
            "EN: I will help you whenever needed\n",
            "FR(hat): je vais t' aider\n",
            "EN(back): i 'm going to help\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "afilnYwsCc6O"
      },
      "execution_count": 60,
      "outputs": []
    }
  ]
}