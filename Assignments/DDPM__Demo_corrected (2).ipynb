{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v_LEw9DDRzg",
        "outputId": "3fa24247-0886-46f7-a176-6ad5272d4c62",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.35.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.1.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision einops timm tqdm matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from typing import List\n",
        "import math\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nap0V8n7Dpxk",
        "outputId": "70d82775-f955-42b8-8987-7cc1462813e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Returns a [B, D] vector for each timestep.\n",
        "    CHANGES:-\n",
        "      - on-the-fly computation (no precomputed [T, D, 1, 1] table)\n",
        "      - returns a vector; per-block Linear projects it to channels\n",
        "    \"\"\"\n",
        "    def __init__(self, time_steps: int, embed_dim: int):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
        "        device = t.device\n",
        "        half = self.embed_dim // 2\n",
        "        freqs = torch.exp(-math.log(10000) * torch.arange(0, half, device=device) / max(half - 1, 1))\n",
        "        arg = t.float().unsqueeze(1) * freqs.unsqueeze(0)          # [B, half]\n",
        "        emb = torch.cat([torch.sin(arg), torch.cos(arg)], dim=1)   # [B, 2*half]\n",
        "        if self.embed_dim % 2 == 1:\n",
        "            emb = F.pad(emb, (0, 1))\n",
        "        return emb"
      ],
      "metadata": {
        "id": "oJETHAYQDuqK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    GroupNorm + SiLU, two 3x3 convs, residual skip (1x1 when channels change).\n",
        "    Time vector is projected (Linear) and added as a channel bias after conv1.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int, time_emb_dim: int, dropout_prob: float = 0.0):\n",
        "        super().__init__()\n",
        "        g1 = 32 if in_channels % 32 == 0 else 1\n",
        "        g2 = 32 if out_channels % 32 == 0 else 1\n",
        "        self.norm1 = nn.GroupNorm(g1, in_channels)\n",
        "        self.act = nn.SiLU()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        self.time_proj = nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, out_channels))  # persistent\n",
        "\n",
        "        self.norm2 = nn.GroupNorm(g2, out_channels)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        self.skip = nn.Identity() if in_channels == out_channels else nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t_vec: torch.Tensor) -> torch.Tensor:\n",
        "        h = self.act(self.norm1(x))\n",
        "        h = self.conv1(h)\n",
        "        h = h + self.time_proj(t_vec)[..., None, None]    # add time as bias\n",
        "        h = self.act(self.norm2(h))\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "        return h + self.skip(x)"
      ],
      "metadata": {
        "id": "QalN8eVMD1eL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"Self-attention block for DDPM.\"\"\"\n",
        "    def __init__(self, channels: int, num_heads: int = 1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.norm = nn.GroupNorm(32, channels)\n",
        "        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1, bias=False)\n",
        "        self.proj = nn.Conv2d(channels, channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: [B, C, H, W]\n",
        "        B, C, H, W = x.shape\n",
        "        h = self.norm(x)\n",
        "        qkv = self.qkv(h)  # [B, 3*C, H, W]\n",
        "\n",
        "        # Split channels into 3 parts for q, k, v\n",
        "        q, k, v = rearrange(qkv, 'B (qkv c) H W -> qkv B c H W', qkv=3, c=C)\n",
        "\n",
        "        # Reshape to [B, num_heads, C_per_head, H*W] for attention\n",
        "        q = rearrange(q, 'B (head c_per_head) H W -> B head c_per_head (H W)', head=self.num_heads)\n",
        "        k = rearrange(k, 'B (head c_per_head) H W -> B head c_per_head (H W)', head=self.num_heads)\n",
        "        v = rearrange(v, 'B (head c_per_head) H W -> B head c_per_head (H W)', head=self.num_heads)\n",
        "\n",
        "        # Compute attention scores: [B, num_heads, H*W, H*W]\n",
        "        attn_scores = torch.einsum('bhid,bhjd->bhij', q, k) * (1.0 / math.sqrt(C // self.num_heads))\n",
        "        attn = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # Apply attention to values: [B, num_heads, C_per_head, H*W]\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
        "\n",
        "        # Reshape back to [B, C, H, W]\n",
        "        out = rearrange(out, 'B head c_per_head (H W) -> B (head c_per_head) H W', head=self.num_heads, H=H, W=W)\n",
        "\n",
        "        # Final projection and residual connection\n",
        "        out = self.proj(out)\n",
        "        return x + out\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    \"\"\"Downsampling block.\"\"\"\n",
        "    def __init__(self, channels: int):\n",
        "        super().__init__()\n",
        "        # Use Conv2d with stride 2 for downsampling\n",
        "        self.conv = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.conv(x)\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    \"\"\"Upsampling block.\"\"\"\n",
        "    def __init__(self, channels: int):\n",
        "        super().__init__()\n",
        "        # Use ConvTranspose2d with stride 2 for upsampling\n",
        "        self.conv = nn.ConvTranspose2d(channels, channels, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    def __init__(self, base_channels=64, channel_mults=[1, 2, 4, 8], input_channels=1, output_channels=1, time_steps=1000, time_emb_dim=256, dropout_prob=0.1, num_heads=1):\n",
        "        super().__init__()\n",
        "        self.time_steps = time_steps\n",
        "        self.time_emb = SinusoidalEmbeddings(time_steps, time_emb_dim)\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.Linear(time_emb_dim, time_emb_dim * 4),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_emb_dim * 4, time_emb_dim)\n",
        "        )\n",
        "\n",
        "        self.initial_conv = nn.Conv2d(input_channels, base_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        channels = [base_channels * mult for mult in channel_mults]\n",
        "        # Add base_channels at the beginning for the first input\n",
        "        all_channels = [base_channels] + channels\n",
        "        in_out_channels = list(zip(all_channels[:-1], all_channels[1:]))\n",
        "\n",
        "        # Downsampling path\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.downsamples = nn.ModuleList()\n",
        "        # The paper typically uses two ResBlocks per resolution level, followed by Downsample\n",
        "        # Attention is placed after the second ResBlock at 16x16 resolution\n",
        "        for i, (in_ch, out_ch) in enumerate(in_out_channels):\n",
        "            self.downs.append(ResBlock(in_ch, out_ch, time_emb_dim, dropout_prob))\n",
        "            self.downs.append(ResBlock(out_ch, out_ch, time_emb_dim, dropout_prob))\n",
        "            # Add attention block after the second ResBlock at the 16x16 resolution level\n",
        "            # Assuming the 16x16 resolution corresponds to the second set of blocks (index 1)\n",
        "            if i == 1: # 32 -> 16 resolution\n",
        "                self.downs.append(AttentionBlock(out_ch, num_heads))\n",
        "            else:\n",
        "                 # Add identity for other levels to maintain list structure\n",
        "                 self.downs.append(nn.Identity())\n",
        "            # Add downsample layer except for the last level\n",
        "            if i < len(in_out_channels) - 1:\n",
        "                self.downsamples.append(Downsample(out_ch))\n",
        "\n",
        "\n",
        "        # Bottleneck\n",
        "        # The bottleneck input should match the output of the last downsampling block\n",
        "        bottleneck_channels = channels[-1] # This is the output channel of the last downsample\n",
        "        self.mid_block1 = ResBlock(bottleneck_channels, bottleneck_channels, time_emb_dim, dropout_prob)\n",
        "        self.mid_attn = AttentionBlock(bottleneck_channels, num_heads)\n",
        "        self.mid_block2 = ResBlock(bottleneck_channels, bottleneck_channels, time_emb_dim, dropout_prob)\n",
        "\n",
        "        # Upsampling path\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.upsamples = nn.ModuleList()\n",
        "        # Reverse the channel list for upsampling\n",
        "        # Need to consider the concatenated skip connection channels\n",
        "        up_channels_out = list(reversed(all_channels[:-1])) # Output channels for up blocks\n",
        "        up_channels_in = list(reversed(all_channels)) # Input channels *before* concatenation\n",
        "\n",
        "        for i in range(len(up_channels_out)):\n",
        "             # Input channels for up blocks are the current feature map channels + skip connection channels\n",
        "             # The skip connection comes from the corresponding downsampling level output channel\n",
        "             # The current feature map channels are the output channels of the previous upsampling/bottleneck block\n",
        "             # For the first upsample block, the input is from the bottleneck (bottleneck_channels)\n",
        "             # For subsequent upsample blocks, the input is the output of the previous up block (up_channels_out[i-1])\n",
        "             current_up_channels = bottleneck_channels if i == 0 else up_channels_out[i-1]\n",
        "             skip_channels = up_channels_out[i] # Skip connection comes from the same resolution level in down path\n",
        "             in_ch_up = current_up_channels + skip_channels\n",
        "             out_ch_up = up_channels_out[i]\n",
        "\n",
        "             self.ups.append(ResBlock(in_ch_up, out_ch_up, time_emb_dim, dropout_prob))\n",
        "             self.ups.append(ResBlock(out_ch_up, out_ch_up, time_emb_dim, dropout_prob))\n",
        "             # Add attention block after the second ResBlock at the 16x16 resolution level\n",
        "             # Assuming the 16x16 resolution corresponds to the second set of up blocks (index 1)\n",
        "             if i == len(up_channels_out) - 2: # 4 -> 8 -> 16 resolution (second to last up block)\n",
        "                  self.ups.append(AttentionBlock(out_ch_up, num_heads))\n",
        "             else:\n",
        "                 # Add identity for other levels\n",
        "                  self.ups.append(nn.Identity())\n",
        "             # Add upsample layer except for the last level\n",
        "             if i < len(up_channels_out) - 1:\n",
        "                 self.upsamples.append(Upsample(out_ch_up))\n",
        "\n",
        "\n",
        "        self.final_norm = nn.GroupNorm(32, base_channels)\n",
        "        self.final_act = nn.SiLU()\n",
        "        self.final_conv = nn.Conv2d(base_channels, output_channels, kernel_size=3, padding=1)\n",
        "\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # x: [B, C, H, W]\n",
        "        # t: [B]\n",
        "\n",
        "        t_emb = self.time_emb(t) # [B, time_emb_dim]\n",
        "        t_emb = self.time_mlp(t_emb) # [B, time_emb_dim]\n",
        "\n",
        "        h = self.initial_conv(x)\n",
        "        residuals = [h] # Store initial input for first skip connection\n",
        "\n",
        "        # Downsampling path\n",
        "        # Iterate through sets of (ResBlock, ResBlock, Attention, Downsample)\n",
        "        for i in range(len(self.downsamples)):\n",
        "            h = self.downs[3*i](h, t_emb) # First ResBlock\n",
        "            h = self.downs[3*i + 1](h, t_emb) # Second ResBlock\n",
        "            h = self.downs[3*i + 2](h) # Attention or Identity\n",
        "            residuals.append(h) # Store output before downsampling for skip connection\n",
        "            h = self.downsamples[i](h) # Downsample\n",
        "\n",
        "\n",
        "        # Bottleneck\n",
        "        h = self.mid_block1(h, t_emb)\n",
        "        h = self.mid_attn(h)\n",
        "        h = self.mid_block2(h, t_emb)\n",
        "\n",
        "        # Upsampling path\n",
        "        # Need to use residuals from the downsampling path in reverse order\n",
        "        # The number of upsample blocks is one less than the number of downsample blocks + bottleneck\n",
        "        # The residuals are from the output of the second ResBlock + Attention before downsampling\n",
        "        residuals = residuals[::-1] # Reverse residuals\n",
        "\n",
        "        for i in range(len(self.upsamples)):\n",
        "            h = self.upsamples[i](h) # Upsample\n",
        "            # Concatenate with the corresponding residual from the downsampling path\n",
        "            # The residual index corresponds to the upsampling level\n",
        "            h = torch.cat([h, residuals[i+1]], dim=1) # Skip connection: current h + residual from down path\n",
        "            h = self.ups[3*i](h, t_emb) # First ResBlock\n",
        "            h = self.ups[3*i + 1](h, t_emb) # Second ResBlock\n",
        "            h = self.ups[3*i + 2](h) # Attention or Identity\n",
        "\n",
        "        # Final output (after the last upsampling block, which brings it back to original resolution)\n",
        "        # The last residual connection is from the initial convolution\n",
        "        # The last up block input should be concatenation of upsampled feature and the initial conv output\n",
        "        # The last element in reversed residuals is the output of the initial conv.\n",
        "        h = torch.cat([h, residuals[-1]], dim=1) # Concatenate with the first residual\n",
        "\n",
        "        h = self.final_norm(h)\n",
        "        h = self.final_act(h)\n",
        "        output = self.final_conv(h)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "zcAhamT8D4s-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DDPM uses a U-Net with four resolutions for 32×32 (32→16→8→4 and back), two residual blocks per resolution, and skip connections via concatenation.\n",
        "Whereas teh given code never downsamples/upsamples and adds skips instead of concatenating."
      ],
      "metadata": {
        "id": "9eUclQpxgPFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Missing self-attention** at 16×16: DDPM paper inserts attention at the 16×16 feature map between conv blocks. Given model has no attention"
      ],
      "metadata": {
        "id": "RCsy7ucXgYHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time conditioning: Paper feeds Transformer sinusoidal time embedding (via MLP) into each residual block using a learned linear projection. Given ResBlock creates a brand-new Conv2d inside forward() (non-persistent) and injects time differently."
      ],
      "metadata": {
        "id": "gPEhE4h7ghW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalization/activations**: Paper uses GroupNorm throughout with the PixelCNN++-style U-Net; SiLU/Swish is standard in faithful implementations. Given blocks use ReLU and dynamic group counts that often reduce to group size 1"
      ],
      "metadata": {
        "id": "EPU7aQyNgqFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SinusoidalEmbeddings**\n",
        "\n",
        "UPDATED: compute sinusoidal emb on the fly; return [B, D] (vector), not [B, D, 1, 1].\n",
        "\n",
        "Reason: paper feeds a vector time embedding to every residual block and projects it there.\n",
        "\n",
        "**ResBlock**\n",
        "\n",
        "UPDATED: remove the Conv2d created inside forward(); add a persistent nn.Linear(time_dim→out_ch) and add this after conv1 as a bias.\n",
        "- paper conditions the U-Net on the timestep inside every residual block, which means a learnable projection that turns the time vector into a per-channel bias for that block.\n",
        "\n",
        "UPDATED: GroupNorm + SiLU activations; proper residual skip with 1×1 when channels change.\n",
        "\n",
        "**UNET**\n",
        "\n",
        "UPDATED:  the 32→16→8→4 hierarchy with Downsample/Upsample.\n",
        "\n",
        "UPDATED: concatenate skip features (not add).\n",
        "\n",
        "UPDATED: inserted AttentionBlock at 16×16.\n",
        "\n",
        "UPDATED: added a small MLP over the sinusoidal time embedding and pass the resulting vector to every ResBlock\n",
        "\n",
        "Implemented the hierarchical structure with explicit Downsample and Upsample layers to achieve the 32->16->8->4 and back resolution changes.\n",
        "\n",
        "- Introduced AttentionBlocks and placed them at the 16x16 resolution levels in both the downsampling and upsampling paths, as specified in the paper.\n",
        "Modified the skip connections to use concatenation (torch.cat) instead of addition, combining the feature maps from the downsampling path with the upsampling path.\n",
        "\n",
        "- Added an MLP (time_mlp) after the SinusoidalEmbeddings to process the time vector before it is fed into the ResBlocks.\n",
        "\n",
        "- Adjusted the forward pass logic to correctly apply the sequence of ResBlocks, Attention (where applicable), and Downsample/Upsample operations, and to handle the concatenation of skip connections."
      ],
      "metadata": {
        "id": "xUoE4xcEiS0Z"
      }
    }
  ]
}